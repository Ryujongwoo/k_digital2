{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53408883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde17af",
   "metadata": {},
   "source": [
    "인공 신경 망(Artifical Neural Network, ANN)\n",
    "\n",
    "신경망의 개념  \n",
    "다층 퍼셉트론(multi-layer perceptron)은 이름 그대로 퍼셉트론의 층이 여러 개라는 뜻으로 다층 퍼셉트론은 기존의 데이터 공간을 변형함으로써 기존의 하나의 퍼셉트론으로 해결할 수 없었던 문제를 해결할 수 있게 되는 것이다. 이처럼 다수의 뉴런을 사용해서 만든 것을 인공 신경망이라고 하며 줄여서 신경망이라 부른다.\n",
    "\n",
    "<img src=\"./images/ann01.png/\" width=\"500\"/>\n",
    "\n",
    "입력층(input layer)에 입력된 데이터는 출력층(output layer)에 도달하기 전에 은닉층(hidden layer)이라는 층을 거친 후 출력층에 도달한다. 은닉층은 아나의 층뿐만 아니라 다수의 층어로 정할 수 있고 딥러닝이라는 이름은 이 은닉층의 깊이가 깊다는 뜻에서 나온 이름이다. 입력층의 노드 개수는 피쳐 개수와 동일하고 출력층의 노드 개수는 분류하려는 클래스의 수와 같다.\n",
    "\n",
    "출력층의 각 노드가 나타내는 수는 해당 클래스에 대한 점수이며 점수가 높을수록 해당 클래스에 속할 확률이 높다는 의미이고 최종적으로 점수가 가장 높은 클래스를 선정한다.\n",
    "\n",
    "신경망 함수는 $f(x) = f_3(f_2(f_1(x)))$와 같은 합성 함수 형태로 표현된다. $f_1$을 첫 번째 신경망 층, $f_2$을 두 번째 신경망 층, $f_3$을 세 번째 신경망 층 이라고 생각하면 된다. 신경망 층이 깊을수록 함수의 개수는 많아지고 합성 함수도 복잡해진다. 이때 합성 함수의 개수는 모형의 깊이(depth)를 의미하는데, 딥러닝에서 Deep이라는 용어가 바로 여기서 나온 용어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b510a64",
   "metadata": {},
   "source": [
    "오차 역전파(back propagation)  \n",
    "다층 퍼셉트론에서 최적값을 찾아가는 과정은 오차 역전파 방법을 사용한다.\n",
    "\n",
    "<img src=\"./images/ann02.png/\" width=\"500\"/>\n",
    "\n",
    "입력층 - 은닉층 - 출력층 순서대로 흘러가는 것을 순전파(forward propagation)라고 하고 반대로 역전파는 출력층 - 은닉층 - 입력층 순서대로 반대로 거슬러 올라가는 방향이다. 오차 역전파를 기반으로 가중치를 수정한 후 더 좋은 성능을 낼 수 있도록 모델을 개선한다.\n",
    "\n",
    "활성화 함수(Activation Function)  \n",
    "활성화 함수는 딥러닝에서 입력값과 가중치, 편향을 계산해서 해당 노드를 활성화 할지를 결정하는 함수이다. 딥러링에서는 결과값을 결정하는 여러가지의 활성화 함수가 존재한다.\n",
    "\n",
    "■ 계단 함수(Step Function)  \n",
    "계단 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "0, \\; x \\leq 0 \\\\\n",
    "1, \\; x > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "계단 함수는 입력값이 0 이하일 경우 0을 출력하고, 0을 초과할 때만 1을 출력한다. 계단 함수의 출력은 0 또는 1로 오직 두 가지 값만 가진다는 특징이 있다.\n",
    "\n",
    "<img src=\"./images/ann03.png/\" width=\"400\"/>\n",
    "\n",
    "입력값 x가 0을 기준으로 0을 넘기 전과 후의 값에 따라 출력값이 바뀌는 것을 알 수 있다. 계단 함수는 사용하기 간단하다는 장점이 있디만 미분이 가능하지 않다는 단점이 있다.\n",
    "\n",
    "■ 부호 함수(Sign Function)  \n",
    "계단 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "1, \\; x > 0 \\\\\n",
    "0, \\; x = 0 \\\\\n",
    "-1, \\; x < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "부호 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"./images/ann04.png/\" width=\"400\"/>\n",
    "\n",
    "계단 함수와 비슷하게 생겼지만, 0 또는 1값만 출력하는 계단 함수와는 달리 -1, 0, 1 값을 출력하는 것을 알 수 있다.\n",
    "\n",
    "■ 시그모이드(Sigmoid Function)  \n",
    "시그모이드 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "시그모이드 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"./images/ann05.png/\" width=\"400\"/>\n",
    "\n",
    "시그모이드 함수는 0과 1 사이의 값을 출력한다.  \n",
    "시그모이드 함수를 딥러닝에 적용했을 때 단점은 학습하는 과정에서 미분을 반복하면 기울기 값이 매우 작아지는 기울기 소실 문제(vanishing gradient problem)가 발생할 수 있다는 것이다. 시그모이드 함수에서 $x$ 값이 지나치게 크거나 작을 경우, 미분값이 0에 가까워지고, 이는 기울기 소실 문제를 발생시켜 학습 속도가 느려질 수 있다.\n",
    "\n",
    "■ 하이퍼볼릭 탄젠트 함수(Hyperbolic Tngent Function, tanh)  \n",
    "하이퍼볼릭 탄젠트 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \\frac{exp(x) - exp(x)}{exp(x) + exp(x)}$$\n",
    "\n",
    "시그모이드 함수의 출력 범위는 0부터 1 사이인 반면에 하이퍼볼릭 탄젠트 함수의 출력 범위는 -1부터 1 사이이다.\n",
    "\n",
    "<img src=\"./images/ann06.png/\" width=\"400\"/>\n",
    "\n",
    "■ 렐루 함수(Rectified Linear Function, ReLu)  \n",
    "렐루 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "0, \\; x \\leq 0 \\\\\n",
    "x, \\; x > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "위 식은 아래와 같이 한 줄로도 표현할 수 있다.\n",
    "\n",
    "$$\\phi(x) = max(x, 0)$$\n",
    "\n",
    "렐루 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"./images/ann07.png/\" width=\"500\"/>\n",
    "\n",
    "렐루 함수는 $x$ 값이 0 이하라면 0을 출력하고, 양수이면 $x$ 값을 그대로 출력한다. 렐루 함수는 계단 함수, 부호 함수, 시그모이드 함수, 하이퍼볼릭 탄젠트 함수와는 다르게 출력값의 상한선이 없다는 특징이 있다.\n",
    "\n",
    "■ 리키 렐루 함수(Leaky ReLu Function)  \n",
    "렐루 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \n",
    "\\begin{cases}\n",
    "ax, \\; x \\leq 0 \\\\\n",
    "x, \\; x > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "위 함수에서 $a \\leq 1$ 이라면 아래와 같이 쓸 수 있다.\n",
    "\n",
    "$$\\phi(x) = max(x, ax)$$\n",
    "\n",
    "렐루 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"./images/ann08.png/\" width=\"400\"/>\n",
    "\n",
    "■ 항등 함수(Identity Function)  \n",
    "항등 함수 또는 선형 함수(Linear Function)라고도 부르는 이 함수는 입력과 출력이 같은 값을 가진다. 주로 회귀 문제에서의 출력층 활성화 함수로 사용된다.  \n",
    "항등 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) =x$$\n",
    "\n",
    "항등 함수를 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<img src=\"./images/ann09.png/\" width=\"300\"/>\n",
    "\n",
    "■ 소프트맥스 함수(Softmax Function)  \n",
    "소프트맥스 함수는 주로 최종 출력층에 사용되는 활성화 함수이다. 만약 해결해야 할 문제가 회귀 문제라면 출력층에 항등 함수를 쓰고, 분류 문제일 경우에는 소프트맥스 함수를 사용한다.  \n",
    "소프트맥스 함수는 아래와 같은 식을 따른다.\n",
    "\n",
    "$$\\phi(x) = \\frac{exp(x_k)}{\\sum_{i=1}^n exp(x_i)}$$\n",
    "\n",
    "소프트맥스 함수를 사용할 때 위 식을 그대로 사용할 경우 오버플로우 문제가 발생할 수 있다. 오버플로우는 출력값이 컴퓨터가 표현할 수 있는 수의 한계를 초과하는 문제를 말한다. 예를 들어 $x_k = 1000$일 때, $exp(1000)$는 매우 큰 수가 되므로 계산하는 것이 불가능하다. 이 문제를 해결하기 위해 소프트맥스 함수를 아래와 같이 변형해서 쓰기도 한다.\n",
    "\n",
    "$$\\phi(x) = \\frac{exp(x_k + C)}{\\sum_{i=1}^n exp(x_i + C)}$$\n",
    "\n",
    "소프트맥스 함수에서 지수 연산을 할 때, 입력값에 임의의 상수 $C$를 더하거나 빼서 오버플로우 문제를 해결한다. 임의의 상수 $C$는 일반적으로 입력값의 최대값을 이용한다. 입력값이 999, 1000, 1001이라 하면 입력값의 최대값은 1001이므로 임의의 상수 $C$는 1001이 된다. $exp(999)$, $exp(1000)$, $exp(1001)$을 구해야 해서 오버플로우가 발생할 수 있지만, 입력값의 최대값을 빼주면 $exp(-2)$, $exp(-1)$, $exp(0)$과 같이 연산이 편해진다.\n",
    "\n",
    "<img src=\"./images/ann10.png/\" width=\"700\"/>\n",
    "\n",
    "배치 정규화(Batch Normalization)  \n",
    "배치 정규화는 해당층의 값의 분포를 변경하는 방법으로, 평균과 분산을 고정시키는 방법이다. 배치 정규화를 이용하면 기울기 소실을 줄임으로써 신경망의 학습 속도를 향상시킬 수 있다는 장점이 있다.\n",
    "\n",
    "드롭아웃(Dropout)  \n",
    "드롭아웃은 신경망의 모든 노드를 사용하는 것이 아닌, 일부 노드를 사용하는 방법이다.\n",
    "\n",
    "<img src=\"./images/ann11.png/\" width=\"900\"/>\n",
    "\n",
    "왼쪽 그림은 드롭아웃을 적용하기 전 신경망이고, 오른쪽 그림이 드롭아웃을 적용한 신경망이다.\n",
    "\n",
    "드롭아웃은 신경망에서 노드를 일시적으로 제거하는 방법이다. 어떤 노드를 신경망에서 제거할지는 각 층에서 무작위로 선택된다. 드롭아웃을 적용하면 신경망의 노드 숫자가 줄어들고 이에 따라 연산량도 줄어들고 오버피팅을 방지할 수 있다는 장점이 있다.\n",
    "\n",
    "텐서플로우 2.x 소개  \n",
    "텐서플로우는 파이썬을 이용해 딥러닝 학습시 사용하는 라이브러리이다. 텐서플로우를 활용하면 신경망을 기본으로 하는 딥러닝에서 다양한 신경망 관련 연산을 처리할 수 있어서 편리하다.\n",
    "\n",
    "텐서플로우를 이용해 신경망 구조를 만드는 방법은 크개 시퀀스 API를 사용하는 방법과 함수형 API를 사용하는 방법이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1581d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32, 32, 100)       200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32, 32, 50)        5050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32, 32, 5)         255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 시퀀스(Sequential) API를 사용하는 방법\n",
    "# 텐서플로우가 제공하는 Sequential 객체를 선언 후 Sequential 모델에 add() 메소드로 layer를 추가해서 쌓아올린다.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# units 속성으로 layer의 출력 차원을 지정하고 activation 속성으로 활성화 함수를 지정해서 Dense layer를 만든다.\n",
    "# Dense layer의 Param은 (입력 차원 + 바이어스) * 출력 차원로 결정된다.\n",
    "model.add(Dense(input_shape=(32, 32, 1), units=100, activation='relu')) # 입력 layer\n",
    "model.add(Dense(units=50, activation='relu')) # 히든 layer\n",
    "model.add(Dense(units=5, activation='softmax')) # 출력 layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394fef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32, 32, 100)       200       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32, 32, 50)        5050      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32, 32, 5)         255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 함수형 API를 사용하는 방법\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "\n",
    "input_layer = Input(shape=(32, 32, 1))\n",
    "x = Dense(units=100, activation='relu')(input_layer) # 입력 layer\n",
    "x = Dense(units=50, activation='relu')(x) # 히든 layer\n",
    "# 위 코드는 은닉층과 활성화 함수를 분리해서 아래와 같이 쓸 수 있다.\n",
    "# x = Dense(units=50)(x)\n",
    "# x = Activation('relu')(x)\n",
    "output_layer = Dense(units=5, activation='softmax')(x) # 출력 layer\n",
    "\n",
    "model2 = Model(input_layer, output_layer)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c59bd",
   "metadata": {},
   "source": [
    "모델 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f45016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장은 save() 메소드를 사용하고 h5 파일로 저장한다.\n",
    "# h5 파일은 hdf5 파일을 의미하는데 Hierarchical Data Format Version 5의 줄인말로 대용량 데이터를 \n",
    "# 저장하기 위한 파일 포맷이다.\n",
    "model.save('./data/ann_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b047376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32, 32, 100)       200       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32, 32, 50)        5050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32, 32, 5)         255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,505\n",
      "Trainable params: 5,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# h5 파일 불러오기\n",
    "from tensorflow.keras.models import load_model\n",
    "ann_model = load_model('./data/ann_model.h5')\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f9e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a0536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
